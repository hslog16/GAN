{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras DC gan MINOR FINAL.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "S-385Ame08Rh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "22861ed7-154a-40f4-8ca7-3ef77d413d53",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525410479819,
          "user_tz": -330,
          "elapsed": 14096,
          "user": {
            "displayName": "Himanshu Sharma",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "103631155484277162666"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget \"https://doc-0s-84-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/audtgi65fb7h3uqsf14bcbiigedi4lcs/1525406400000/13182073909007362810/*/0B7EVK8r0v71pZjFTYXZWM3FlRnM?e=download\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: wildcards not supported in HTTP.\r\n",
            "--2018-05-04 05:07:47--  https://doc-0s-84-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/audtgi65fb7h3uqsf14bcbiigedi4lcs/1525406400000/13182073909007362810/*/0B7EVK8r0v71pZjFTYXZWM3FlRnM?e=download\r\n",
            "Resolving doc-0s-84-docs.googleusercontent.com (doc-0s-84-docs.googleusercontent.com)... 74.125.141.132, 2607:f8b0:400c:c06::84\r\n",
            "Connecting to doc-0s-84-docs.googleusercontent.com (doc-0s-84-docs.googleusercontent.com)|74.125.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘0B7EVK8r0v71pZjFTYXZWM3FlRnM?e=download’\n",
            "\n",
            "0B7EVK8r0v71pZjFTYX     [            <=>     ]   1.34G   114MB/s    in 11s     \n",
            "\n",
            "2018-05-04 05:07:59 (125 MB/s) - ‘0B7EVK8r0v71pZjFTYXZWM3FlRnM?e=download’ saved [1443490838]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RXaPoEBY1hjs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!mv \"0B7EVK8r0v71pZjFTYXZWM3FlRnM?e=download\" \"celeba.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gTnaaZI11xec",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!mv img_align_celeba data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o_ex0Gys1z6h",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def get_batch(image_files, width, height, mode):\n",
        "        data_batch = np.array(\n",
        "            [get_image(sample_file, width, height, mode) for sample_file in image_files])\n",
        "\n",
        "        return data_batch\n",
        "\n",
        "def get_image(image_path, width, height, mode):\n",
        "    \n",
        "        image = Image.open(image_path)\n",
        "        # image = image.resize([width, height], Image.BILINEAR)\n",
        "        if image.size != (width, height):  \n",
        "        # Remove most pixels that aren't part of a face\n",
        "            face_width = face_height = 108\n",
        "            j = (image.size[0] - face_width) // 2\n",
        "            i = (image.size[1] - face_height) // 2\n",
        "            image = image.crop([j, i, j + face_width, i + face_height])\n",
        "            image = image.resize([width, height])\n",
        "    \n",
        "        return np.array(image.convert(mode))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l3cBuhnO14AO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import os\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4GeZzdm-17lI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "9245be6a-30d0-4e64-99ca-021b4eb59856",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525396990709,
          "user_tz": -330,
          "elapsed": 1553,
          "user": {
            "displayName": "Himanshu Sharma",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "103631155484277162666"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:9: UserWarning: \n",
            "This call to matplotlib.use() has no effect because the backend has already\n",
            "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
            "or matplotlib.backends is imported for the first time.\n",
            "\n",
            "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
            "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
            "    \"__main__\", fname, loader, pkg_name)\n",
            "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
            "    exec code in run_globals\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 657, in launch_instance\n",
            "    app.initialize(argv)\n",
            "  File \"<decorator-gen-121>\", line 2, in initialize\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 87, in catch_config_error\n",
            "    return method(app, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 462, in initialize\n",
            "    self.init_gui_pylab()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 403, in init_gui_pylab\n",
            "    InteractiveShellApp.init_gui_pylab(self)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/shellapp.py\", line 213, in init_gui_pylab\n",
            "    r = enable(key)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n",
            "    pt.activate_matplotlib(backend)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n",
            "    matplotlib.pyplot.switch_backend(backend)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
            "    matplotlib.use(newbackend, warn=False, force=True)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
            "    reload(sys.modules['matplotlib.backends'])\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
            "    line for line in traceback.format_stack()\n",
            "\n",
            "\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "HAa8Hu7j2A__",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def build_generator(noise=(100,)):\n",
        "    input = Input(noise)\n",
        "    layer = Dense(128 * 8 * 8, activation=\"relu\")(input)\n",
        "    layer = Reshape((8, 8, 128))(layer)\n",
        "    layer = BatchNormalization(momentum=0.8)(layer) # check the formula\n",
        "    layer = UpSampling2D()(layer)\n",
        "    layer = Conv2D(128, kernel_size=3, padding=\"same\")(layer)\n",
        "    layer = Activation(\"relu\")(layer)\n",
        "    layer = BatchNormalization(momentum=0.8)(layer)\n",
        "    layer = UpSampling2D()(layer)\n",
        "    layer = Conv2D(64, kernel_size=3, padding=\"same\")(layer)\n",
        "    layer = Activation(\"relu\")(layer)\n",
        "    layer = BatchNormalization(momentum=0.8)(layer)\n",
        "    layer = Conv2D(3, kernel_size=3, padding=\"same\")(layer)\n",
        "    out = Activation(\"tanh\")(layer) # test karke dekha tha tanh is advised by the author itself\n",
        "    model = Model(input, out)\n",
        "    print(\" Generator Model Summary \")\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jl7YCj-U2aWF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def build_discriminator(input_img):\n",
        "    input = Input(input_img)\n",
        "    layer =Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(input)\n",
        "    layer = LeakyReLU(alpha=0.2)(layer)\n",
        "    layer = Dropout(0.25)(layer)\n",
        "    layer = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(layer)\n",
        "    layer = (LeakyReLU(alpha=0.2))(layer)\n",
        "    layer = Dropout(0.25)(layer)\n",
        "    layer = BatchNormalization(momentum=0.8)(layer)\n",
        "    layer = Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(layer)\n",
        "    layer = LeakyReLU(alpha=0.2)(layer)\n",
        "    layer = Dropout(0.25)(layer)\n",
        "    layer = BatchNormalization(momentum=0.8)(layer)\n",
        "    layer = Conv2D(256, kernel_size=3, strides=1, padding=\"same\")(layer)\n",
        "    layer = LeakyReLU(alpha=0.2)(layer)\n",
        "    layer = Dropout(0.25)(layer)\n",
        "    layer = Flatten()(layer)\n",
        "    out = Dense(1, activation='sigmoid')(layer)\n",
        "\n",
        "    model = Model(input, out)\n",
        "    print(\" Discriminator Model Summary \")\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MAAk5TP23UUU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def train(generator, discriminator, stacked, epochs=2000, batchNo=128):\n",
        "    data_dir = './data'\n",
        "    train_arr = get_batch(glob(os.path.join(data_dir, '*.jpg'))[:10000], 32, 32, 'RGB')\n",
        "    \n",
        "    train_arr = (train_arr.astype(np.float32) - 127.5) / 127.5\n",
        "    \n",
        "    num_batches = int(train_arr.shape[0] / float(batchNo))\n",
        "    \n",
        "    print('Number of examples: ', train_arr.shape[0])\n",
        "    print('Number of Batches: ', num_batches)\n",
        "    print('Number of epochs: ', epochs)\n",
        "\n",
        "    half_batchNo = int(batchNo / 2)\n",
        "    \n",
        "    accuracy = []\n",
        "\n",
        "\n",
        "    for epoch in range(epochs + 1):\n",
        "        print(\"Epoch: \" + str(epoch))\n",
        "        for batch in range(num_batches):\n",
        "            print(\"Batch: \" + str(batch) + \"/\" + str(num_batches))\n",
        "    \n",
        "            # noise images for the batch\n",
        "            noise = np.random.normal(0, 1, (half_batchNo, 100))\n",
        "        \n",
        "            generateImg = generator.predict(noise)\n",
        "            zero_Label = np.zeros((half_batchNo, 1))\n",
        "    \n",
        "            # real images for batch\n",
        "            idx = np.random.randint(0, train_arr.shape[0], half_batchNo)\n",
        "            datasetImg = train_arr[idx]\n",
        "            one_Label = np.ones((half_batchNo, 1))\n",
        "    \n",
        "            # Train the discriminator (real classified as ones and generated as zeros)\n",
        "            disc_TrainR = discriminator.train_on_batch(datasetImg, one_Label)\n",
        "            disc_TrainF = discriminator.train_on_batch(generateImg, zero_Label)\n",
        "            mean_Loss = 0.5 * np.add(disc_TrainR, disc_TrainF)\n",
        "    \n",
        "            noise = np.random.normal(0, 1, (batchNo, 100))\n",
        "            # Train the generator\n",
        "            gen_Train = stacked.train_on_batch(noise, np.ones((batchNo, 1)))\n",
        "    \n",
        "            # Plot the progress\n",
        "            print(\"Epoch %d Batch %d/%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %\n",
        "                  (epoch, batch, num_batches, mean_Loss[0], 100 * mean_Loss[1], gen_Train))\n",
        "            \n",
        "            accuracy.append([ ((epoch+1)*batch) , (100*mean_Loss[1])] )\n",
        "            \n",
        "            if batch % 50 == 0:\n",
        "                save_imgs(generator, epoch, batch)\n",
        "            \n",
        "            accuracy_a = np.array(accuracy)\n",
        "    \n",
        "    plt.plot(accuracy_a[:,0], accuracy_a[:,1], label=\"Discriminator ACCURACY \")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy of Discriminator over epochs for CelebA dataset On Convolutional Model')\n",
        "    plt.grid(True)\n",
        "    plt.show()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M9t1StSG7rLJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def save_imgs(generator, epoch, batch):\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, 100))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = (1/2.5) * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i, j].imshow(gen_imgs[cnt, :, :, :])\n",
        "            axs[i, j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"images/Celeb_%d_%d.png\" % (epoch, batch))\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K7Zve-Qi77Vi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HWeH1UQi79tE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def build_models():\n",
        "\n",
        "   #start karte hai 0.002 k learning rate se \n",
        "   # what is beta_1 ????\n",
        "    Gopt = Adam(lr=0.0002, beta_1=0.5)\n",
        "    Dopt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\n",
        "    discriminator = build_discriminator(input_img=(32, 32, 3))\n",
        "    discriminator.compile(loss='binary_crossentropy',\n",
        "                               optimizer=Dopt,\n",
        "                               metrics=['accuracy'])\n",
        "\n",
        "    generator = build_generator()\n",
        "    generator.compile(loss='binary_crossentropy', optimizer=Gopt)\n",
        "\n",
        "    x = Input(shape=(100,))\n",
        "    img = generator(x)\n",
        "    discriminator.trainable = False \n",
        "    real = discriminator(img)\n",
        "    stacked = Model(x, real)\n",
        "    stacked.compile(loss='binary_crossentropy', optimizer=Gopt)\n",
        "    return generator, discriminator, stacked"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eubj46wDEoMc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 15976
        },
        "outputId": "e3e763bb-24d9-47e8-cdac-3c6b48281433",
        "executionInfo": {
          "status": "error",
          "timestamp": 1525397391946,
          "user_tz": -330,
          "elapsed": 64938,
          "user": {
            "displayName": "Himanshu Sharma",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "103631155484277162666"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    generator, discriminator, stacked = build_models()\n",
        "    train(generator, discriminator, stacked, epochs=7, batchNo=32)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Discriminator Model Summary \n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 16, 16, 32)        896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 8, 8, 64)          18496     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 4097      \n",
            "=================================================================\n",
            "Total params: 393,281\n",
            "Trainable params: 392,897\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            " Generator Model Summary \n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 8192)              827392    \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2 (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 32, 32, 64)        73792     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 32, 32, 3)         1731      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 32, 32, 3)         0         \n",
            "=================================================================\n",
            "Total params: 1,051,779\n",
            "Trainable params: 1,051,139\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n",
            "('Number of examples: ', 10000)\n",
            "('Number of Batches: ', 312)\n",
            "('Number of epochs: ', 7)\n",
            "Epoch: 0\n",
            "Batch: 0/312\n",
            "Epoch 0 Batch 0/312 [D loss: 0.565321, acc.: 68.75%] [G loss: 0.566988]\n",
            "Batch: 1/312\n",
            "Epoch 0 Batch 1/312 [D loss: 0.325426, acc.: 96.88%] [G loss: 0.339376]\n",
            "Batch: 2/312\n",
            "Epoch 0 Batch 2/312 [D loss: 0.224035, acc.: 96.88%] [G loss: 0.308520]\n",
            "Batch: 3/312\n",
            "Epoch 0 Batch 3/312 [D loss: 0.159454, acc.: 96.88%] [G loss: 0.179726]\n",
            "Batch: 4/312\n",
            "Epoch 0 Batch 4/312 [D loss: 0.141558, acc.: 100.00%] [G loss: 0.134958]\n",
            "Batch: 5/312\n",
            "Epoch 0 Batch 5/312 [D loss: 0.078537, acc.: 100.00%] [G loss: 0.093387]\n",
            "Batch: 6/312\n",
            "Epoch 0 Batch 6/312 [D loss: 0.056941, acc.: 100.00%] [G loss: 0.098790]\n",
            "Batch: 7/312\n",
            "Epoch 0 Batch 7/312 [D loss: 0.070595, acc.: 100.00%] [G loss: 0.063399]\n",
            "Batch: 8/312\n",
            "Epoch 0 Batch 8/312 [D loss: 0.052348, acc.: 100.00%] [G loss: 0.085388]\n",
            "Batch: 9/312\n",
            "Epoch 0 Batch 9/312 [D loss: 0.050287, acc.: 100.00%] [G loss: 0.056114]\n",
            "Batch: 10/312\n",
            "Epoch 0 Batch 10/312 [D loss: 0.057580, acc.: 100.00%] [G loss: 0.044905]\n",
            "Batch: 11/312\n",
            "Epoch 0 Batch 11/312 [D loss: 0.041654, acc.: 100.00%] [G loss: 0.052660]\n",
            "Batch: 12/312\n",
            "Epoch 0 Batch 12/312 [D loss: 0.026574, acc.: 100.00%] [G loss: 0.053017]\n",
            "Batch: 13/312\n",
            "Epoch 0 Batch 13/312 [D loss: 0.033538, acc.: 100.00%] [G loss: 0.029585]\n",
            "Batch: 14/312\n",
            "Epoch 0 Batch 14/312 [D loss: 0.027615, acc.: 100.00%] [G loss: 0.035319]\n",
            "Batch: 15/312\n",
            "Epoch 0 Batch 15/312 [D loss: 0.022118, acc.: 100.00%] [G loss: 0.029234]\n",
            "Batch: 16/312\n",
            "Epoch 0 Batch 16/312 [D loss: 0.020464, acc.: 100.00%] [G loss: 0.027271]\n",
            "Batch: 17/312\n",
            "Epoch 0 Batch 17/312 [D loss: 0.015008, acc.: 100.00%] [G loss: 0.021036]\n",
            "Batch: 18/312\n",
            "Epoch 0 Batch 18/312 [D loss: 0.013147, acc.: 100.00%] [G loss: 0.020705]\n",
            "Batch: 19/312\n",
            "Epoch 0 Batch 19/312 [D loss: 0.021277, acc.: 100.00%] [G loss: 0.020890]\n",
            "Batch: 20/312\n",
            "Epoch 0 Batch 20/312 [D loss: 0.022781, acc.: 100.00%] [G loss: 0.015513]\n",
            "Batch: 21/312\n",
            "Epoch 0 Batch 21/312 [D loss: 0.014716, acc.: 100.00%] [G loss: 0.016033]\n",
            "Batch: 22/312\n",
            "Epoch 0 Batch 22/312 [D loss: 0.010612, acc.: 100.00%] [G loss: 0.012073]\n",
            "Batch: 23/312\n",
            "Epoch 0 Batch 23/312 [D loss: 0.010451, acc.: 100.00%] [G loss: 0.019956]\n",
            "Batch: 24/312\n",
            "Epoch 0 Batch 24/312 [D loss: 0.007142, acc.: 100.00%] [G loss: 0.013238]\n",
            "Batch: 25/312\n",
            "Epoch 0 Batch 25/312 [D loss: 0.010182, acc.: 100.00%] [G loss: 0.014266]\n",
            "Batch: 26/312\n",
            "Epoch 0 Batch 26/312 [D loss: 0.012605, acc.: 100.00%] [G loss: 0.016090]\n",
            "Batch: 27/312\n",
            "Epoch 0 Batch 27/312 [D loss: 0.009399, acc.: 100.00%] [G loss: 0.014747]\n",
            "Batch: 28/312\n",
            "Epoch 0 Batch 28/312 [D loss: 0.005605, acc.: 100.00%] [G loss: 0.010353]\n",
            "Batch: 29/312\n",
            "Epoch 0 Batch 29/312 [D loss: 0.010450, acc.: 100.00%] [G loss: 0.009254]\n",
            "Batch: 30/312\n",
            "Epoch 0 Batch 30/312 [D loss: 0.006606, acc.: 100.00%] [G loss: 0.013668]\n",
            "Batch: 31/312\n",
            "Epoch 0 Batch 31/312 [D loss: 0.007293, acc.: 100.00%] [G loss: 0.009517]\n",
            "Batch: 32/312\n",
            "Epoch 0 Batch 32/312 [D loss: 0.005324, acc.: 100.00%] [G loss: 0.007362]\n",
            "Batch: 33/312\n",
            "Epoch 0 Batch 33/312 [D loss: 0.007044, acc.: 100.00%] [G loss: 0.007210]\n",
            "Batch: 34/312\n",
            "Epoch 0 Batch 34/312 [D loss: 0.004887, acc.: 100.00%] [G loss: 0.005555]\n",
            "Batch: 35/312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Batch 35/312 [D loss: 0.005496, acc.: 100.00%] [G loss: 0.011371]\n",
            "Batch: 36/312\n",
            "Epoch 0 Batch 36/312 [D loss: 0.009369, acc.: 100.00%] [G loss: 0.007002]\n",
            "Batch: 37/312\n",
            "Epoch 0 Batch 37/312 [D loss: 0.005915, acc.: 100.00%] [G loss: 0.006011]\n",
            "Batch: 38/312\n",
            "Epoch 0 Batch 38/312 [D loss: 0.006326, acc.: 100.00%] [G loss: 0.006908]\n",
            "Batch: 39/312\n",
            "Epoch 0 Batch 39/312 [D loss: 0.005544, acc.: 100.00%] [G loss: 0.006358]\n",
            "Batch: 40/312\n",
            "Epoch 0 Batch 40/312 [D loss: 0.006393, acc.: 100.00%] [G loss: 0.007348]\n",
            "Batch: 41/312\n",
            "Epoch 0 Batch 41/312 [D loss: 0.005540, acc.: 100.00%] [G loss: 0.010831]\n",
            "Batch: 42/312\n",
            "Epoch 0 Batch 42/312 [D loss: 0.004402, acc.: 100.00%] [G loss: 0.007001]\n",
            "Batch: 43/312\n",
            "Epoch 0 Batch 43/312 [D loss: 0.003600, acc.: 100.00%] [G loss: 0.007308]\n",
            "Batch: 44/312\n",
            "Epoch 0 Batch 44/312 [D loss: 0.003334, acc.: 100.00%] [G loss: 0.005486]\n",
            "Batch: 45/312\n",
            "Epoch 0 Batch 45/312 [D loss: 0.003489, acc.: 100.00%] [G loss: 0.005463]\n",
            "Batch: 46/312\n",
            "Epoch 0 Batch 46/312 [D loss: 0.004057, acc.: 100.00%] [G loss: 0.006144]\n",
            "Batch: 47/312\n",
            "Epoch 0 Batch 47/312 [D loss: 0.002703, acc.: 100.00%] [G loss: 0.007414]\n",
            "Batch: 48/312\n",
            "Epoch 0 Batch 48/312 [D loss: 0.004228, acc.: 100.00%] [G loss: 0.004725]\n",
            "Batch: 49/312\n",
            "Epoch 0 Batch 49/312 [D loss: 0.003578, acc.: 100.00%] [G loss: 0.005015]\n",
            "Batch: 50/312\n",
            "Epoch 0 Batch 50/312 [D loss: 0.004274, acc.: 100.00%] [G loss: 0.004759]\n",
            "Batch: 51/312\n",
            "Epoch 0 Batch 51/312 [D loss: 0.003668, acc.: 100.00%] [G loss: 0.003849]\n",
            "Batch: 52/312\n",
            "Epoch 0 Batch 52/312 [D loss: 0.004814, acc.: 100.00%] [G loss: 0.005821]\n",
            "Batch: 53/312\n",
            "Epoch 0 Batch 53/312 [D loss: 0.002684, acc.: 100.00%] [G loss: 0.004852]\n",
            "Batch: 54/312\n",
            "Epoch 0 Batch 54/312 [D loss: 0.003110, acc.: 100.00%] [G loss: 0.003861]\n",
            "Batch: 55/312\n",
            "Epoch 0 Batch 55/312 [D loss: 0.002249, acc.: 100.00%] [G loss: 0.002939]\n",
            "Batch: 56/312\n",
            "Epoch 0 Batch 56/312 [D loss: 0.002234, acc.: 100.00%] [G loss: 0.003192]\n",
            "Batch: 57/312\n",
            "Epoch 0 Batch 57/312 [D loss: 0.002141, acc.: 100.00%] [G loss: 0.003281]\n",
            "Batch: 58/312\n",
            "Epoch 0 Batch 58/312 [D loss: 0.002144, acc.: 100.00%] [G loss: 0.005320]\n",
            "Batch: 59/312\n",
            "Epoch 0 Batch 59/312 [D loss: 0.003463, acc.: 100.00%] [G loss: 0.003400]\n",
            "Batch: 60/312\n",
            "Epoch 0 Batch 60/312 [D loss: 0.001510, acc.: 100.00%] [G loss: 0.004030]\n",
            "Batch: 61/312\n",
            "Epoch 0 Batch 61/312 [D loss: 0.002188, acc.: 100.00%] [G loss: 0.004455]\n",
            "Batch: 62/312\n",
            "Epoch 0 Batch 62/312 [D loss: 0.002184, acc.: 100.00%] [G loss: 0.003204]\n",
            "Batch: 63/312\n",
            "Epoch 0 Batch 63/312 [D loss: 0.002168, acc.: 100.00%] [G loss: 0.003091]\n",
            "Batch: 64/312\n",
            "Epoch 0 Batch 64/312 [D loss: 0.003931, acc.: 100.00%] [G loss: 0.002339]\n",
            "Batch: 65/312\n",
            "Epoch 0 Batch 65/312 [D loss: 0.003310, acc.: 100.00%] [G loss: 0.004747]\n",
            "Batch: 66/312\n",
            "Epoch 0 Batch 66/312 [D loss: 0.002196, acc.: 100.00%] [G loss: 0.002507]\n",
            "Batch: 67/312\n",
            "Epoch 0 Batch 67/312 [D loss: 0.002609, acc.: 100.00%] [G loss: 0.002830]\n",
            "Batch: 68/312\n",
            "Epoch 0 Batch 68/312 [D loss: 0.001634, acc.: 100.00%] [G loss: 0.003087]\n",
            "Batch: 69/312\n",
            "Epoch 0 Batch 69/312 [D loss: 0.002276, acc.: 100.00%] [G loss: 0.002762]\n",
            "Batch: 70/312\n",
            "Epoch 0 Batch 70/312 [D loss: 0.001437, acc.: 100.00%] [G loss: 0.003111]\n",
            "Batch: 71/312\n",
            "Epoch 0 Batch 71/312 [D loss: 0.002515, acc.: 100.00%] [G loss: 0.003616]\n",
            "Batch: 72/312\n",
            "Epoch 0 Batch 72/312 [D loss: 0.001558, acc.: 100.00%] [G loss: 0.002908]\n",
            "Batch: 73/312\n",
            "Epoch 0 Batch 73/312 [D loss: 0.001161, acc.: 100.00%] [G loss: 0.002361]\n",
            "Batch: 74/312\n",
            "Epoch 0 Batch 74/312 [D loss: 0.001490, acc.: 100.00%] [G loss: 0.002462]\n",
            "Batch: 75/312\n",
            "Epoch 0 Batch 75/312 [D loss: 0.001890, acc.: 100.00%] [G loss: 0.002140]\n",
            "Batch: 76/312\n",
            "Epoch 0 Batch 76/312 [D loss: 0.001238, acc.: 100.00%] [G loss: 0.001953]\n",
            "Batch: 77/312\n",
            "Epoch 0 Batch 77/312 [D loss: 0.001876, acc.: 100.00%] [G loss: 0.001908]\n",
            "Batch: 78/312\n",
            "Epoch 0 Batch 78/312 [D loss: 0.001527, acc.: 100.00%] [G loss: 0.003284]\n",
            "Batch: 79/312\n",
            "Epoch 0 Batch 79/312 [D loss: 0.002176, acc.: 100.00%] [G loss: 0.002635]\n",
            "Batch: 80/312\n",
            "Epoch 0 Batch 80/312 [D loss: 0.001260, acc.: 100.00%] [G loss: 0.001809]\n",
            "Batch: 81/312\n",
            "Epoch 0 Batch 81/312 [D loss: 0.001591, acc.: 100.00%] [G loss: 0.002329]\n",
            "Batch: 82/312\n",
            "Epoch 0 Batch 82/312 [D loss: 0.001184, acc.: 100.00%] [G loss: 0.002721]\n",
            "Batch: 83/312\n",
            "Epoch 0 Batch 83/312 [D loss: 0.001670, acc.: 100.00%] [G loss: 0.001935]\n",
            "Batch: 84/312\n",
            "Epoch 0 Batch 84/312 [D loss: 0.001156, acc.: 100.00%] [G loss: 0.002206]\n",
            "Batch: 85/312\n",
            "Epoch 0 Batch 85/312 [D loss: 0.001481, acc.: 100.00%] [G loss: 0.002456]\n",
            "Batch: 86/312\n",
            "Epoch 0 Batch 86/312 [D loss: 0.001166, acc.: 100.00%] [G loss: 0.001778]\n",
            "Batch: 87/312\n",
            "Epoch 0 Batch 87/312 [D loss: 0.001017, acc.: 100.00%] [G loss: 0.001747]\n",
            "Batch: 88/312\n",
            "Epoch 0 Batch 88/312 [D loss: 0.001811, acc.: 100.00%] [G loss: 0.001726]\n",
            "Batch: 89/312\n",
            "Epoch 0 Batch 89/312 [D loss: 0.001458, acc.: 100.00%] [G loss: 0.002371]\n",
            "Batch: 90/312\n",
            "Epoch 0 Batch 90/312 [D loss: 0.001685, acc.: 100.00%] [G loss: 0.002446]\n",
            "Batch: 91/312\n",
            "Epoch 0 Batch 91/312 [D loss: 0.000832, acc.: 100.00%] [G loss: 0.001578]\n",
            "Batch: 92/312\n",
            "Epoch 0 Batch 92/312 [D loss: 0.000950, acc.: 100.00%] [G loss: 0.001465]\n",
            "Batch: 93/312\n",
            "Epoch 0 Batch 93/312 [D loss: 0.001314, acc.: 100.00%] [G loss: 0.001498]\n",
            "Batch: 94/312\n",
            "Epoch 0 Batch 94/312 [D loss: 0.001091, acc.: 100.00%] [G loss: 0.001322]\n",
            "Batch: 95/312\n",
            "Epoch 0 Batch 95/312 [D loss: 0.001040, acc.: 100.00%] [G loss: 0.001673]\n",
            "Batch: 96/312\n",
            "Epoch 0 Batch 96/312 [D loss: 0.002324, acc.: 100.00%] [G loss: 0.001774]\n",
            "Batch: 97/312\n",
            "Epoch 0 Batch 97/312 [D loss: 0.001321, acc.: 100.00%] [G loss: 0.001436]\n",
            "Batch: 98/312\n",
            "Epoch 0 Batch 98/312 [D loss: 0.001075, acc.: 100.00%] [G loss: 0.001112]\n",
            "Batch: 99/312\n",
            "Epoch 0 Batch 99/312 [D loss: 0.001419, acc.: 100.00%] [G loss: 0.001931]\n",
            "Batch: 100/312\n",
            "Epoch 0 Batch 100/312 [D loss: 0.001051, acc.: 100.00%] [G loss: 0.003841]\n",
            "Batch: 101/312\n",
            "Epoch 0 Batch 101/312 [D loss: 0.000941, acc.: 100.00%] [G loss: 0.001360]\n",
            "Batch: 102/312\n",
            "Epoch 0 Batch 102/312 [D loss: 0.000661, acc.: 100.00%] [G loss: 0.000980]\n",
            "Batch: 103/312\n",
            "Epoch 0 Batch 103/312 [D loss: 0.001276, acc.: 100.00%] [G loss: 0.001625]\n",
            "Batch: 104/312\n",
            "Epoch 0 Batch 104/312 [D loss: 0.000867, acc.: 100.00%] [G loss: 0.001236]\n",
            "Batch: 105/312\n",
            "Epoch 0 Batch 105/312 [D loss: 0.000836, acc.: 100.00%] [G loss: 0.001388]\n",
            "Batch: 106/312\n",
            "Epoch 0 Batch 106/312 [D loss: 0.001043, acc.: 100.00%] [G loss: 0.001363]\n",
            "Batch: 107/312\n",
            "Epoch 0 Batch 107/312 [D loss: 0.001133, acc.: 100.00%] [G loss: 0.001658]\n",
            "Batch: 108/312\n",
            "Epoch 0 Batch 108/312 [D loss: 0.000816, acc.: 100.00%] [G loss: 0.001789]\n",
            "Batch: 109/312\n",
            "Epoch 0 Batch 109/312 [D loss: 0.000699, acc.: 100.00%] [G loss: 0.001499]\n",
            "Batch: 110/312\n",
            "Epoch 0 Batch 110/312 [D loss: 0.000859, acc.: 100.00%] [G loss: 0.001795]\n",
            "Batch: 111/312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Batch 111/312 [D loss: 0.001103, acc.: 100.00%] [G loss: 0.001257]\n",
            "Batch: 112/312\n",
            "Epoch 0 Batch 112/312 [D loss: 0.001048, acc.: 100.00%] [G loss: 0.000702]\n",
            "Batch: 113/312\n",
            "Epoch 0 Batch 113/312 [D loss: 0.000839, acc.: 100.00%] [G loss: 0.001227]\n",
            "Batch: 114/312\n",
            "Epoch 0 Batch 114/312 [D loss: 0.000855, acc.: 100.00%] [G loss: 0.001043]\n",
            "Batch: 115/312\n",
            "Epoch 0 Batch 115/312 [D loss: 0.001504, acc.: 100.00%] [G loss: 0.001171]\n",
            "Batch: 116/312\n",
            "Epoch 0 Batch 116/312 [D loss: 0.000584, acc.: 100.00%] [G loss: 0.000929]\n",
            "Batch: 117/312\n",
            "Epoch 0 Batch 117/312 [D loss: 0.001262, acc.: 100.00%] [G loss: 0.001556]\n",
            "Batch: 118/312\n",
            "Epoch 0 Batch 118/312 [D loss: 0.000575, acc.: 100.00%] [G loss: 0.001019]\n",
            "Batch: 119/312\n",
            "Epoch 0 Batch 119/312 [D loss: 0.000891, acc.: 100.00%] [G loss: 0.001051]\n",
            "Batch: 120/312\n",
            "Epoch 0 Batch 120/312 [D loss: 0.000790, acc.: 100.00%] [G loss: 0.001207]\n",
            "Batch: 121/312\n",
            "Epoch 0 Batch 121/312 [D loss: 0.000537, acc.: 100.00%] [G loss: 0.000740]\n",
            "Batch: 122/312\n",
            "Epoch 0 Batch 122/312 [D loss: 0.000841, acc.: 100.00%] [G loss: 0.000978]\n",
            "Batch: 123/312\n",
            "Epoch 0 Batch 123/312 [D loss: 0.000535, acc.: 100.00%] [G loss: 0.001063]\n",
            "Batch: 124/312\n",
            "Epoch 0 Batch 124/312 [D loss: 0.000729, acc.: 100.00%] [G loss: 0.001230]\n",
            "Batch: 125/312\n",
            "Epoch 0 Batch 125/312 [D loss: 0.000879, acc.: 100.00%] [G loss: 0.000993]\n",
            "Batch: 126/312\n",
            "Epoch 0 Batch 126/312 [D loss: 0.000578, acc.: 100.00%] [G loss: 0.000707]\n",
            "Batch: 127/312\n",
            "Epoch 0 Batch 127/312 [D loss: 0.000666, acc.: 100.00%] [G loss: 0.000784]\n",
            "Batch: 128/312\n",
            "Epoch 0 Batch 128/312 [D loss: 0.001010, acc.: 100.00%] [G loss: 0.000868]\n",
            "Batch: 129/312\n",
            "Epoch 0 Batch 129/312 [D loss: 0.000570, acc.: 100.00%] [G loss: 0.000745]\n",
            "Batch: 130/312\n",
            "Epoch 0 Batch 130/312 [D loss: 0.000474, acc.: 100.00%] [G loss: 0.000731]\n",
            "Batch: 131/312\n",
            "Epoch 0 Batch 131/312 [D loss: 0.000705, acc.: 100.00%] [G loss: 0.000729]\n",
            "Batch: 132/312\n",
            "Epoch 0 Batch 132/312 [D loss: 0.000591, acc.: 100.00%] [G loss: 0.000790]\n",
            "Batch: 133/312\n",
            "Epoch 0 Batch 133/312 [D loss: 0.000812, acc.: 100.00%] [G loss: 0.000899]\n",
            "Batch: 134/312\n",
            "Epoch 0 Batch 134/312 [D loss: 0.000537, acc.: 100.00%] [G loss: 0.000550]\n",
            "Batch: 135/312\n",
            "Epoch 0 Batch 135/312 [D loss: 0.000947, acc.: 100.00%] [G loss: 0.000825]\n",
            "Batch: 136/312\n",
            "Epoch 0 Batch 136/312 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.000880]\n",
            "Batch: 137/312\n",
            "Epoch 0 Batch 137/312 [D loss: 0.000548, acc.: 100.00%] [G loss: 0.000908]\n",
            "Batch: 138/312\n",
            "Epoch 0 Batch 138/312 [D loss: 0.000673, acc.: 100.00%] [G loss: 0.001281]\n",
            "Batch: 139/312\n",
            "Epoch 0 Batch 139/312 [D loss: 0.000792, acc.: 100.00%] [G loss: 0.001035]\n",
            "Batch: 140/312\n",
            "Epoch 0 Batch 140/312 [D loss: 0.000600, acc.: 100.00%] [G loss: 0.000797]\n",
            "Batch: 141/312\n",
            "Epoch 0 Batch 141/312 [D loss: 0.000474, acc.: 100.00%] [G loss: 0.000744]\n",
            "Batch: 142/312\n",
            "Epoch 0 Batch 142/312 [D loss: 0.000554, acc.: 100.00%] [G loss: 0.000745]\n",
            "Batch: 143/312\n",
            "Epoch 0 Batch 143/312 [D loss: 0.000564, acc.: 100.00%] [G loss: 0.000596]\n",
            "Batch: 144/312\n",
            "Epoch 0 Batch 144/312 [D loss: 0.000418, acc.: 100.00%] [G loss: 0.000647]\n",
            "Batch: 145/312\n",
            "Epoch 0 Batch 145/312 [D loss: 0.000522, acc.: 100.00%] [G loss: 0.000901]\n",
            "Batch: 146/312\n",
            "Epoch 0 Batch 146/312 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.000683]\n",
            "Batch: 147/312\n",
            "Epoch 0 Batch 147/312 [D loss: 0.000813, acc.: 100.00%] [G loss: 0.000701]\n",
            "Batch: 148/312\n",
            "Epoch 0 Batch 148/312 [D loss: 0.000743, acc.: 100.00%] [G loss: 0.000794]\n",
            "Batch: 149/312\n",
            "Epoch 0 Batch 149/312 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.001021]\n",
            "Batch: 150/312\n",
            "Epoch 0 Batch 150/312 [D loss: 0.000651, acc.: 100.00%] [G loss: 0.000612]\n",
            "Batch: 151/312\n",
            "Epoch 0 Batch 151/312 [D loss: 0.000592, acc.: 100.00%] [G loss: 0.001050]\n",
            "Batch: 152/312\n",
            "Epoch 0 Batch 152/312 [D loss: 0.000420, acc.: 100.00%] [G loss: 0.000539]\n",
            "Batch: 153/312\n",
            "Epoch 0 Batch 153/312 [D loss: 0.000630, acc.: 100.00%] [G loss: 0.000759]\n",
            "Batch: 154/312\n",
            "Epoch 0 Batch 154/312 [D loss: 0.000413, acc.: 100.00%] [G loss: 0.000589]\n",
            "Batch: 155/312\n",
            "Epoch 0 Batch 155/312 [D loss: 0.000355, acc.: 100.00%] [G loss: 0.000637]\n",
            "Batch: 156/312\n",
            "Epoch 0 Batch 156/312 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.001097]\n",
            "Batch: 157/312\n",
            "Epoch 0 Batch 157/312 [D loss: 0.000635, acc.: 100.00%] [G loss: 0.000579]\n",
            "Batch: 158/312\n",
            "Epoch 0 Batch 158/312 [D loss: 0.000495, acc.: 100.00%] [G loss: 0.001061]\n",
            "Batch: 159/312\n",
            "Epoch 0 Batch 159/312 [D loss: 0.000371, acc.: 100.00%] [G loss: 0.000884]\n",
            "Batch: 160/312\n",
            "Epoch 0 Batch 160/312 [D loss: 0.000426, acc.: 100.00%] [G loss: 0.000536]\n",
            "Batch: 161/312\n",
            "Epoch 0 Batch 161/312 [D loss: 0.000593, acc.: 100.00%] [G loss: 0.000992]\n",
            "Batch: 162/312\n",
            "Epoch 0 Batch 162/312 [D loss: 0.000572, acc.: 100.00%] [G loss: 0.001036]\n",
            "Batch: 163/312\n",
            "Epoch 0 Batch 163/312 [D loss: 0.000394, acc.: 100.00%] [G loss: 0.000490]\n",
            "Batch: 164/312\n",
            "Epoch 0 Batch 164/312 [D loss: 0.000602, acc.: 100.00%] [G loss: 0.000580]\n",
            "Batch: 165/312\n",
            "Epoch 0 Batch 165/312 [D loss: 0.000378, acc.: 100.00%] [G loss: 0.000461]\n",
            "Batch: 166/312\n",
            "Epoch 0 Batch 166/312 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.000406]\n",
            "Batch: 167/312\n",
            "Epoch 0 Batch 167/312 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.000578]\n",
            "Batch: 168/312\n",
            "Epoch 0 Batch 168/312 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.000837]\n",
            "Batch: 169/312\n",
            "Epoch 0 Batch 169/312 [D loss: 0.000565, acc.: 100.00%] [G loss: 0.000680]\n",
            "Batch: 170/312\n",
            "Epoch 0 Batch 170/312 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.000322]\n",
            "Batch: 171/312\n",
            "Epoch 0 Batch 171/312 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.000477]\n",
            "Batch: 172/312\n",
            "Epoch 0 Batch 172/312 [D loss: 0.000426, acc.: 100.00%] [G loss: 0.000458]\n",
            "Batch: 173/312\n",
            "Epoch 0 Batch 173/312 [D loss: 0.000720, acc.: 100.00%] [G loss: 0.000487]\n",
            "Batch: 174/312\n",
            "Epoch 0 Batch 174/312 [D loss: 0.000458, acc.: 100.00%] [G loss: 0.000579]\n",
            "Batch: 175/312\n",
            "Epoch 0 Batch 175/312 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.000657]\n",
            "Batch: 176/312\n",
            "Epoch 0 Batch 176/312 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.000336]\n",
            "Batch: 177/312\n",
            "Epoch 0 Batch 177/312 [D loss: 0.001036, acc.: 100.00%] [G loss: 0.000493]\n",
            "Batch: 178/312\n",
            "Epoch 0 Batch 178/312 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.000477]\n",
            "Batch: 179/312\n",
            "Epoch 0 Batch 179/312 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.000347]\n",
            "Batch: 180/312\n",
            "Epoch 0 Batch 180/312 [D loss: 0.000475, acc.: 100.00%] [G loss: 0.000419]\n",
            "Batch: 181/312\n",
            "Epoch 0 Batch 181/312 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.000746]\n",
            "Batch: 182/312\n",
            "Epoch 0 Batch 182/312 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.000393]\n",
            "Batch: 183/312\n",
            "Epoch 0 Batch 183/312 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.000432]\n",
            "Batch: 184/312\n",
            "Epoch 0 Batch 184/312 [D loss: 0.000489, acc.: 100.00%] [G loss: 0.000529]\n",
            "Batch: 185/312\n",
            "Epoch 0 Batch 185/312 [D loss: 0.000482, acc.: 100.00%] [G loss: 0.000375]\n",
            "Batch: 186/312\n",
            "Epoch 0 Batch 186/312 [D loss: 0.000331, acc.: 100.00%] [G loss: 0.000331]\n",
            "Batch: 187/312\n",
            "Epoch 0 Batch 187/312 [D loss: 0.001210, acc.: 100.00%] [G loss: 0.000444]\n",
            "Batch: 188/312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Batch 188/312 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.000376]\n",
            "Batch: 189/312\n",
            "Epoch 0 Batch 189/312 [D loss: 0.000579, acc.: 100.00%] [G loss: 0.000649]\n",
            "Batch: 190/312\n",
            "Epoch 0 Batch 190/312 [D loss: 0.000419, acc.: 100.00%] [G loss: 0.000392]\n",
            "Batch: 191/312\n",
            "Epoch 0 Batch 191/312 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.000505]\n",
            "Batch: 192/312\n",
            "Epoch 0 Batch 192/312 [D loss: 0.000297, acc.: 100.00%] [G loss: 0.000570]\n",
            "Batch: 193/312\n",
            "Epoch 0 Batch 193/312 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.000551]\n",
            "Batch: 194/312\n",
            "Epoch 0 Batch 194/312 [D loss: 0.000397, acc.: 100.00%] [G loss: 0.000377]\n",
            "Batch: 195/312\n",
            "Epoch 0 Batch 195/312 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.000594]\n",
            "Batch: 196/312\n",
            "Epoch 0 Batch 196/312 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.000501]\n",
            "Batch: 197/312\n",
            "Epoch 0 Batch 197/312 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.000474]\n",
            "Batch: 198/312\n",
            "Epoch 0 Batch 198/312 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.000581]\n",
            "Batch: 199/312\n",
            "Epoch 0 Batch 199/312 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.000328]\n",
            "Batch: 200/312\n",
            "Epoch 0 Batch 200/312 [D loss: 0.000524, acc.: 100.00%] [G loss: 0.000360]\n",
            "Batch: 201/312\n",
            "Epoch 0 Batch 201/312 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.000411]\n",
            "Batch: 202/312\n",
            "Epoch 0 Batch 202/312 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.000461]\n",
            "Batch: 203/312\n",
            "Epoch 0 Batch 203/312 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.000354]\n",
            "Batch: 204/312\n",
            "Epoch 0 Batch 204/312 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.000372]\n",
            "Batch: 205/312\n",
            "Epoch 0 Batch 205/312 [D loss: 0.000294, acc.: 100.00%] [G loss: 0.000765]\n",
            "Batch: 206/312\n",
            "Epoch 0 Batch 206/312 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.000323]\n",
            "Batch: 207/312\n",
            "Epoch 0 Batch 207/312 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.000323]\n",
            "Batch: 208/312\n",
            "Epoch 0 Batch 208/312 [D loss: 0.000600, acc.: 100.00%] [G loss: 0.000288]\n",
            "Batch: 209/312\n",
            "Epoch 0 Batch 209/312 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.000246]\n",
            "Batch: 210/312\n",
            "Epoch 0 Batch 210/312 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.000455]\n",
            "Batch: 211/312\n",
            "Epoch 0 Batch 211/312 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.000431]\n",
            "Batch: 212/312\n",
            "Epoch 0 Batch 212/312 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.000383]\n",
            "Batch: 213/312\n",
            "Epoch 0 Batch 213/312 [D loss: 0.000328, acc.: 100.00%] [G loss: 0.000238]\n",
            "Batch: 214/312\n",
            "Epoch 0 Batch 214/312 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.000576]\n",
            "Batch: 215/312\n",
            "Epoch 0 Batch 215/312 [D loss: 0.000447, acc.: 100.00%] [G loss: 0.000426]\n",
            "Batch: 216/312\n",
            "Epoch 0 Batch 216/312 [D loss: 0.000764, acc.: 100.00%] [G loss: 0.000367]\n",
            "Batch: 217/312\n",
            "Epoch 0 Batch 217/312 [D loss: 0.000301, acc.: 100.00%] [G loss: 0.000361]\n",
            "Batch: 218/312\n",
            "Epoch 0 Batch 218/312 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.000296]\n",
            "Batch: 219/312\n",
            "Epoch 0 Batch 219/312 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.000618]\n",
            "Batch: 220/312\n",
            "Epoch 0 Batch 220/312 [D loss: 0.000241, acc.: 100.00%] [G loss: 0.000440]\n",
            "Batch: 221/312\n",
            "Epoch 0 Batch 221/312 [D loss: 0.000425, acc.: 100.00%] [G loss: 0.000411]\n",
            "Batch: 222/312\n",
            "Epoch 0 Batch 222/312 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.000348]\n",
            "Batch: 223/312\n",
            "Epoch 0 Batch 223/312 [D loss: 0.000311, acc.: 100.00%] [G loss: 0.000217]\n",
            "Batch: 224/312\n",
            "Epoch 0 Batch 224/312 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.000386]\n",
            "Batch: 225/312\n",
            "Epoch 0 Batch 225/312 [D loss: 0.000391, acc.: 100.00%] [G loss: 0.000192]\n",
            "Batch: 226/312\n",
            "Epoch 0 Batch 226/312 [D loss: 0.000546, acc.: 100.00%] [G loss: 0.000351]\n",
            "Batch: 227/312\n",
            "Epoch 0 Batch 227/312 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.000364]\n",
            "Batch: 228/312\n",
            "Epoch 0 Batch 228/312 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.000223]\n",
            "Batch: 229/312\n",
            "Epoch 0 Batch 229/312 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.000358]\n",
            "Batch: 230/312\n",
            "Epoch 0 Batch 230/312 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.000331]\n",
            "Batch: 231/312\n",
            "Epoch 0 Batch 231/312 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.000349]\n",
            "Batch: 232/312\n",
            "Epoch 0 Batch 232/312 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.000402]\n",
            "Batch: 233/312\n",
            "Epoch 0 Batch 233/312 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.000323]\n",
            "Batch: 234/312\n",
            "Epoch 0 Batch 234/312 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.000236]\n",
            "Batch: 235/312\n",
            "Epoch 0 Batch 235/312 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.000199]\n",
            "Batch: 236/312\n",
            "Epoch 0 Batch 236/312 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.000231]\n",
            "Batch: 237/312\n",
            "Epoch 0 Batch 237/312 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.000266]\n",
            "Batch: 238/312\n",
            "Epoch 0 Batch 238/312 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000445]\n",
            "Batch: 239/312\n",
            "Epoch 0 Batch 239/312 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.000260]\n",
            "Batch: 240/312\n",
            "Epoch 0 Batch 240/312 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.000344]\n",
            "Batch: 241/312\n",
            "Epoch 0 Batch 241/312 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.000197]\n",
            "Batch: 242/312\n",
            "Epoch 0 Batch 242/312 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000452]\n",
            "Batch: 243/312\n",
            "Epoch 0 Batch 243/312 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.000334]\n",
            "Batch: 244/312\n",
            "Epoch 0 Batch 244/312 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000340]\n",
            "Batch: 245/312\n",
            "Epoch 0 Batch 245/312 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.000414]\n",
            "Batch: 246/312\n",
            "Epoch 0 Batch 246/312 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.000318]\n",
            "Batch: 247/312\n",
            "Epoch 0 Batch 247/312 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.000308]\n",
            "Batch: 248/312\n",
            "Epoch 0 Batch 248/312 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000350]\n",
            "Batch: 249/312\n",
            "Epoch 0 Batch 249/312 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.000222]\n",
            "Batch: 250/312\n",
            "Epoch 0 Batch 250/312 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.000244]\n",
            "Batch: 251/312\n",
            "Epoch 0 Batch 251/312 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.000196]\n",
            "Batch: 252/312\n",
            "Epoch 0 Batch 252/312 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.000255]\n",
            "Batch: 253/312\n",
            "Epoch 0 Batch 253/312 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.000451]\n",
            "Batch: 254/312\n",
            "Epoch 0 Batch 254/312 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.000403]\n",
            "Batch: 255/312\n",
            "Epoch 0 Batch 255/312 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000142]\n",
            "Batch: 256/312\n",
            "Epoch 0 Batch 256/312 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.000192]\n",
            "Batch: 257/312\n",
            "Epoch 0 Batch 257/312 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.000322]\n",
            "Batch: 258/312\n",
            "Epoch 0 Batch 258/312 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.000219]\n",
            "Batch: 259/312\n",
            "Epoch 0 Batch 259/312 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000205]\n",
            "Batch: 260/312\n",
            "Epoch 0 Batch 260/312 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.000268]\n",
            "Batch: 261/312\n",
            "Epoch 0 Batch 261/312 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.000212]\n",
            "Batch: 262/312\n",
            "Epoch 0 Batch 262/312 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000193]\n",
            "Batch: 263/312\n",
            "Epoch 0 Batch 263/312 [D loss: 0.000277, acc.: 100.00%] [G loss: 0.000282]\n",
            "Batch: 264/312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Batch 264/312 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.000414]\n",
            "Batch: 265/312\n",
            "Epoch 0 Batch 265/312 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.000372]\n",
            "Batch: 266/312\n",
            "Epoch 0 Batch 266/312 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.000266]\n",
            "Batch: 267/312\n",
            "Epoch 0 Batch 267/312 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.000159]\n",
            "Batch: 268/312\n",
            "Epoch 0 Batch 268/312 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.000296]\n",
            "Batch: 269/312\n",
            "Epoch 0 Batch 269/312 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000230]\n",
            "Batch: 270/312\n",
            "Epoch 0 Batch 270/312 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.000310]\n",
            "Batch: 271/312\n",
            "Epoch 0 Batch 271/312 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000171]\n",
            "Batch: 272/312\n",
            "Epoch 0 Batch 272/312 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.000175]\n",
            "Batch: 273/312\n",
            "Epoch 0 Batch 273/312 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000195]\n",
            "Batch: 274/312\n",
            "Epoch 0 Batch 274/312 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000192]\n",
            "Batch: 275/312\n",
            "Epoch 0 Batch 275/312 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.000210]\n",
            "Batch: 276/312\n",
            "Epoch 0 Batch 276/312 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.000315]\n",
            "Batch: 277/312\n",
            "Epoch 0 Batch 277/312 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.000225]\n",
            "Batch: 278/312\n",
            "Epoch 0 Batch 278/312 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.000209]\n",
            "Batch: 279/312\n",
            "Epoch 0 Batch 279/312 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000262]\n",
            "Batch: 280/312\n",
            "Epoch 0 Batch 280/312 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.000219]\n",
            "Batch: 281/312\n",
            "Epoch 0 Batch 281/312 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.000174]\n",
            "Batch: 282/312\n",
            "Epoch 0 Batch 282/312 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.000235]\n",
            "Batch: 283/312\n",
            "Epoch 0 Batch 283/312 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000149]\n",
            "Batch: 284/312\n",
            "Epoch 0 Batch 284/312 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.000348]\n",
            "Batch: 285/312\n",
            "Epoch 0 Batch 285/312 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000230]\n",
            "Batch: 286/312\n",
            "Epoch 0 Batch 286/312 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.000251]\n",
            "Batch: 287/312\n",
            "Epoch 0 Batch 287/312 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.000391]\n",
            "Batch: 288/312\n",
            "Epoch 0 Batch 288/312 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000249]\n",
            "Batch: 289/312\n",
            "Epoch 0 Batch 289/312 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000199]\n",
            "Batch: 290/312\n",
            "Epoch 0 Batch 290/312 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.000375]\n",
            "Batch: 291/312\n",
            "Epoch 0 Batch 291/312 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000251]\n",
            "Batch: 292/312\n",
            "Epoch 0 Batch 292/312 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000273]\n",
            "Batch: 293/312\n",
            "Epoch 0 Batch 293/312 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.000290]\n",
            "Batch: 294/312\n",
            "Epoch 0 Batch 294/312 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.000380]\n",
            "Batch: 295/312\n",
            "Epoch 0 Batch 295/312 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.000354]\n",
            "Batch: 296/312\n",
            "Epoch 0 Batch 296/312 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000214]\n",
            "Batch: 297/312\n",
            "Epoch 0 Batch 297/312 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000229]\n",
            "Batch: 298/312\n",
            "Epoch 0 Batch 298/312 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.000212]\n",
            "Batch: 299/312\n",
            "Epoch 0 Batch 299/312 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000184]\n",
            "Batch: 300/312\n",
            "Epoch 0 Batch 300/312 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.000193]\n",
            "Batch: 301/312\n",
            "Epoch 0 Batch 301/312 [D loss: 0.000323, acc.: 100.00%] [G loss: 0.000208]\n",
            "Batch: 302/312\n",
            "Epoch 0 Batch 302/312 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.000180]\n",
            "Batch: 303/312\n",
            "Epoch 0 Batch 303/312 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.000229]\n",
            "Batch: 304/312\n",
            "Epoch 0 Batch 304/312 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000250]\n",
            "Batch: 305/312\n",
            "Epoch 0 Batch 305/312 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.000237]\n",
            "Batch: 306/312\n",
            "Epoch 0 Batch 306/312 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000204]\n",
            "Batch: 307/312\n",
            "Epoch 0 Batch 307/312 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000180]\n",
            "Batch: 308/312\n",
            "Epoch 0 Batch 308/312 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000184]\n",
            "Batch: 309/312\n",
            "Epoch 0 Batch 309/312 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.000177]\n",
            "Batch: 310/312\n",
            "Epoch 0 Batch 310/312 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.000184]\n",
            "Batch: 311/312\n",
            "Epoch 0 Batch 311/312 [D loss: 0.000385, acc.: 100.00%] [G loss: 0.000165]\n",
            "Epoch: 1\n",
            "Batch: 0/312\n",
            "Epoch 1 Batch 0/312 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000146]\n",
            "Batch: 1/312\n",
            "Epoch 1 Batch 1/312 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000230]\n",
            "Batch: 2/312\n",
            "Epoch 1 Batch 2/312 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.000220]\n",
            "Batch: 3/312\n",
            "Epoch 1 Batch 3/312 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000179]\n",
            "Batch: 4/312\n",
            "Epoch 1 Batch 4/312 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000141]\n",
            "Batch: 5/312\n",
            "Epoch 1 Batch 5/312 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000147]\n",
            "Batch: 6/312\n",
            "Epoch 1 Batch 6/312 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000133]\n",
            "Batch: 7/312\n",
            "Epoch 1 Batch 7/312 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000196]\n",
            "Batch: 8/312\n",
            "Epoch 1 Batch 8/312 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000153]\n",
            "Batch: 9/312\n",
            "Epoch 1 Batch 9/312 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.000161]\n",
            "Batch: 10/312\n",
            "Epoch 1 Batch 10/312 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.000180]\n",
            "Batch: 11/312\n",
            "Epoch 1 Batch 11/312 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000190]\n",
            "Batch: 12/312\n",
            "Epoch 1 Batch 12/312 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000263]\n",
            "Batch: 13/312\n",
            "Epoch 1 Batch 13/312 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000190]\n",
            "Batch: 14/312\n",
            "Epoch 1 Batch 14/312 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.000170]\n",
            "Batch: 15/312\n",
            "Epoch 1 Batch 15/312 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000173]\n",
            "Batch: 16/312\n",
            "Epoch 1 Batch 16/312 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000223]\n",
            "Batch: 17/312\n",
            "Epoch 1 Batch 17/312 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.000124]\n",
            "Batch: 18/312\n",
            "Epoch 1 Batch 18/312 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.000170]\n",
            "Batch: 19/312\n",
            "Epoch 1 Batch 19/312 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000144]\n",
            "Batch: 20/312\n",
            "Epoch 1 Batch 20/312 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000152]\n",
            "Batch: 21/312\n",
            "Epoch 1 Batch 21/312 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000195]\n",
            "Batch: 22/312\n",
            "Epoch 1 Batch 22/312 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000130]\n",
            "Batch: 23/312\n",
            "Epoch 1 Batch 23/312 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.000130]\n",
            "Batch: 24/312\n",
            "Epoch 1 Batch 24/312 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000237]\n",
            "Batch: 25/312\n",
            "Epoch 1 Batch 25/312 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000189]\n",
            "Batch: 26/312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 26/312 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000155]\n",
            "Batch: 27/312\n",
            "Epoch 1 Batch 27/312 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000195]\n",
            "Batch: 28/312\n",
            "Epoch 1 Batch 28/312 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000132]\n",
            "Batch: 29/312\n",
            "Epoch 1 Batch 29/312 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000175]\n",
            "Batch: 30/312\n",
            "Epoch 1 Batch 30/312 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.000129]\n",
            "Batch: 31/312\n",
            "Epoch 1 Batch 31/312 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000152]\n",
            "Batch: 32/312\n",
            "Epoch 1 Batch 32/312 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.000212]\n",
            "Batch: 33/312\n",
            "Epoch 1 Batch 33/312 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.000106]\n",
            "Batch: 34/312\n",
            "Epoch 1 Batch 34/312 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000118]\n",
            "Batch: 35/312\n",
            "Epoch 1 Batch 35/312 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000142]\n",
            "Batch: 36/312\n",
            "Epoch 1 Batch 36/312 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.000240]\n",
            "Batch: 37/312\n",
            "Epoch 1 Batch 37/312 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000143]\n",
            "Batch: 38/312\n",
            "Epoch 1 Batch 38/312 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000177]\n",
            "Batch: 39/312\n",
            "Epoch 1 Batch 39/312 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000188]\n",
            "Batch: 40/312\n",
            "Epoch 1 Batch 40/312 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000150]\n",
            "Batch: 41/312\n",
            "Epoch 1 Batch 41/312 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000174]\n",
            "Batch: 42/312\n",
            "Epoch 1 Batch 42/312 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000098]\n",
            "Batch: 43/312\n",
            "Epoch 1 Batch 43/312 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000263]\n",
            "Batch: 44/312\n",
            "Epoch 1 Batch 44/312 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000171]\n",
            "Batch: 45/312\n",
            "Epoch 1 Batch 45/312 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.000108]\n",
            "Batch: 46/312\n",
            "Epoch 1 Batch 46/312 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000182]\n",
            "Batch: 47/312\n",
            "Epoch 1 Batch 47/312 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000136]\n",
            "Batch: 48/312\n",
            "Epoch 1 Batch 48/312 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000118]\n",
            "Batch: 49/312\n",
            "Epoch 1 Batch 49/312 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000125]\n",
            "Batch: 50/312\n",
            "Epoch 1 Batch 50/312 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000148]\n",
            "Batch: 51/312\n",
            "Epoch 1 Batch 51/312 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000144]\n",
            "Batch: 52/312\n",
            "Epoch 1 Batch 52/312 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000102]\n",
            "Batch: 53/312\n",
            "Epoch 1 Batch 53/312 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000170]\n",
            "Batch: 54/312\n",
            "Epoch 1 Batch 54/312 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000113]\n",
            "Batch: 55/312\n",
            "Epoch 1 Batch 55/312 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000130]\n",
            "Batch: 56/312\n",
            "Epoch 1 Batch 56/312 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000146]\n",
            "Batch: 57/312\n",
            "Epoch 1 Batch 57/312 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000181]\n",
            "Batch: 58/312\n",
            "Epoch 1 Batch 58/312 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000097]\n",
            "Batch: 59/312\n",
            "Epoch 1 Batch 59/312 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000203]\n",
            "Batch: 60/312\n",
            "Epoch 1 Batch 60/312 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000106]\n",
            "Batch: 61/312\n",
            "Epoch 1 Batch 61/312 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000136]\n",
            "Batch: 62/312\n",
            "Epoch 1 Batch 62/312 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000112]\n",
            "Batch: 63/312\n",
            "Epoch 1 Batch 63/312 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000146]\n",
            "Batch: 64/312\n",
            "Epoch 1 Batch 64/312 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000153]\n",
            "Batch: 65/312\n",
            "Epoch 1 Batch 65/312 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000095]\n",
            "Batch: 66/312\n",
            "Epoch 1 Batch 66/312 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.000117]\n",
            "Batch: 67/312\n",
            "Epoch 1 Batch 67/312 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000157]\n",
            "Batch: 68/312\n",
            "Epoch 1 Batch 68/312 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000084]\n",
            "Batch: 69/312\n",
            "Epoch 1 Batch 69/312 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000152]\n",
            "Batch: 70/312\n",
            "Epoch 1 Batch 70/312 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000171]\n",
            "Batch: 71/312\n",
            "Epoch 1 Batch 71/312 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000115]\n",
            "Batch: 72/312\n",
            "Epoch 1 Batch 72/312 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.000086]\n",
            "Batch: 73/312\n",
            "Epoch 1 Batch 73/312 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000113]\n",
            "Batch: 74/312\n",
            "Epoch 1 Batch 74/312 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000133]\n",
            "Batch: 75/312\n",
            "Epoch 1 Batch 75/312 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000100]\n",
            "Batch: 76/312\n",
            "Epoch 1 Batch 76/312 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000141]\n",
            "Batch: 77/312\n",
            "Epoch 1 Batch 77/312 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000109]\n",
            "Batch: 78/312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-bd3a4fea86d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchNo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-e05f27b7d504>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(generator, discriminator, stacked, epochs, batchNo)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatchNo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# Train the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mgen_Train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchNo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# Plot the progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "0TLzXlSKFsln",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "5ddd9623-ced2-4284-afbc-60f97c812303",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525397396305,
          "user_tz": -330,
          "elapsed": 2093,
          "user": {
            "displayName": "Himanshu Sharma",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "103631155484277162666"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls images"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Celeb_0_0.png\t Celeb_1_200.png  Celeb_2_50.png   Celeb_4_150.png\r\n",
            "Celeb_0_100.png  Celeb_1_250.png  Celeb_3_0.png    Celeb_4_200.png\r\n",
            "Celeb_0_150.png  Celeb_1_300.png  Celeb_3_100.png  Celeb_4_250.png\r\n",
            "Celeb_0_200.png  Celeb_1_50.png   Celeb_3_150.png  Celeb_4_300.png\r\n",
            "Celeb_0_250.png  Celeb_2_0.png\t  Celeb_3_200.png  Celeb_4_50.png\r\n",
            "Celeb_0_300.png  Celeb_2_100.png  Celeb_3_250.png  Celeb_5_0.png\r\n",
            "Celeb_0_50.png\t Celeb_2_150.png  Celeb_3_300.png  Celeb_5_100.png\r\n",
            "Celeb_1_0.png\t Celeb_2_200.png  Celeb_3_50.png   Celeb_5_50.png\r\n",
            "Celeb_1_100.png  Celeb_2_250.png  Celeb_4_0.png\r\n",
            "Celeb_1_150.png  Celeb_2_300.png  Celeb_4_100.png\r\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}